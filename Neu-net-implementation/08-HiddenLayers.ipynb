{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c5fa6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3d2ca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetworkHidden:\n",
    "    def __init__(self, input_dim, hidden1_dim, hidden2_dim, hidden3_dim, output_dim):\n",
    "        self.weights1 = torch.randn(input_dim, hidden1_dim, dtype=torch.float32, requires_grad=True)\n",
    "        self.bias1    = torch.randn(hidden1_dim, dtype=torch.float, requires_grad=True)\n",
    "        self.weights2 = torch.randn(hidden1_dim, hidden2_dim, dtype=torch.float32, requires_grad=True)\n",
    "        self.bias2    = torch.randn(hidden2_dim, dtype=torch.float, requires_grad=True)\n",
    "        self.weights3 = torch.randn(hidden2_dim, hidden3_dim, dtype=torch.float32, requires_grad=True)\n",
    "        self.bias3    = torch.randn(hidden3_dim, dtype=torch.float, requires_grad=True)\n",
    "        self.weights4 = torch.randn(hidden3_dim, output_dim, dtype=torch.float32, requires_grad=True)\n",
    "        self.bias4    = torch.randn(output_dim, dtype=torch.float, requires_grad=True)\n",
    "        \n",
    "# we defined our class for the neural Network \n",
    "# init is called for the Object \n",
    "# For Each Connection there are weights and biases attached \n",
    "# w1 and b1 , w2 and b2 , w3 and b3 , w_out and b_out.\n",
    "# requires_grad = True \n",
    "# means that torch is aware to track the calculations for Automatic Double Diff that is requires for Gradient Calculations\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = torch.matmul(x, self.weights1) + self.bias1\n",
    "        h1_activated = F.relu(h1)\n",
    "        \n",
    "        h2 = torch.matmul(h1_activated, self.weights2) + self.bias2\n",
    "        h2_activated = F.relu(h2)\n",
    "        \n",
    "        h3 = torch.matmul(h2_activated, self.weights3) + self.bias3\n",
    "        h3_activated = F.relu(h3)\n",
    "        \n",
    "        out = torch.matmul(h3_activated, self.weights4) + self.bias4\n",
    "        out_activated = torch.sigmoid(out)\n",
    "        return out_activated\n",
    "        \n",
    "    def train(self, X, y, epochs=10000, lr=0.01):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X).squeeze()\n",
    "            loss = F.binary_cross_entropy(y_pred, y)\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                self.weights1 -= lr * self.weights1.grad\n",
    "                self.bias1    -= lr * self.bias1.grad\n",
    "                self.weights2 -= lr * self.weights2.grad\n",
    "                self.bias2    -= lr * self.bias2.grad\n",
    "                self.weights3 -= lr * self.weights3.grad\n",
    "                self.bias3    -= lr * self.bias3.grad\n",
    "                self.weights4 -= lr * self.weights4.grad\n",
    "                self.bias4    -= lr * self.bias4.grad\n",
    "                # we will be making the Gradients zero as .grad just adds the gradient to the grad attribute and not replace the, . hence , we have to rplace the grad at the end of each Epoch\n",
    "                self.weights1.grad.zero_()\n",
    "                self.bias1.grad.zero_()\n",
    "                self.weights2.grad.zero_()\n",
    "                self.bias2.grad.zero_()\n",
    "                self.weights3.grad.zero_()\n",
    "                self.bias3.grad.zero_()\n",
    "                self.weights4.grad.zero_()\n",
    "                self.bias4.grad.zero_()\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "            \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "724f7e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6919, -1.3879, -0.3514,  1.5164],\n",
      "        [ 0.4000,  1.0992,  0.3893,  1.4079],\n",
      "        [-1.0785,  0.5306, -0.6706, -0.9550],\n",
      "        [-1.1348,  0.2709, -0.5362,  0.1539],\n",
      "        [-0.7887,  0.2471, -1.7662, -0.1676],\n",
      "        [-0.3692,  0.4702, -1.4871, -0.1875],\n",
      "        [-0.1863, -2.0947,  0.5295,  0.1710],\n",
      "        [-0.2400, -1.2525,  1.4284, -0.1239],\n",
      "        [-1.7365, -1.4579,  0.9034,  1.2238],\n",
      "        [-0.3551, -0.8091,  1.7716, -0.0229]])\n",
      "tensor([1., 1., 1., 0., 0., 0., 1., 1., 0., 1.])\n",
      "Epoch 0: Loss = 4.9552\n",
      "Epoch 100: Loss = 0.3728\n",
      "Epoch 200: Loss = 0.3119\n",
      "Epoch 300: Loss = 0.2509\n",
      "Epoch 400: Loss = 0.2075\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example input and output\n",
    "X = torch.randn(10, 4)  # 10 samples, 4 features (input_dim=4)\n",
    "y = torch.randint(0, 2, (10,), dtype=torch.float32)  # 10 binary labels\n",
    "print(X)\n",
    "print(y)\n",
    "# Create the neural network object\n",
    "model = SimpleNeuralNetworkHidden(\n",
    "    input_dim=4, hidden1_dim=8, hidden2_dim=8, hidden3_dim=4, output_dim=1\n",
    ")\n",
    "\n",
    "# Train the network (this will print the loss every 100 epochs)\n",
    "model.train(X, y, epochs=500, lr=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
